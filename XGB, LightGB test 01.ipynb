{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"},"colab":{"name":"XGB, LightGB test 01.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"o238NiUSK219"},"source":["import numpy as np\n","import pandas as pd\n","import warnings\n","warnings.filterwarnings(action='ignore')\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# import missingno as msno\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.metrics import f1_score\n","\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pQbYwjq5K21-"},"source":["# 데이터 불러오기\n","train = pd.read_csv('~/Downloads/kakr-4th-competition/train.csv')\n","test = pd.read_csv('~/Downloads/kakr-4th-competition/test.csv')\n","sample_submission = pd.read_csv('~/Downloads/kakr-4th-competition/sample_submission.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXvzTX4SK21-"},"source":["# 함수화"]},{"cell_type":"code","metadata":{"id":"hsYMyTBhK21-"},"source":["# 1) column 제거\n","def col_reduction(df):\n","    df.drop(['id','fnlwgt','education','relationship','native_country','workclass'], axis=1, inplace=True)\n","    \n","    return df\n","\n","# 2) marital_status 조정\n","def mar_st(df):\n","    df['marital_status'] = (df['marital_status'] == 'Married-civ-spouse').astype(int)\n","    \n","    return df\n","\n","# 3) capital_gain, loss 조정\n","def capital(df):\n","    df['cap_gain_high'] = (df['capital_gain'] != 0).astype(int)\n","    df['cap_loss_high'] = (df['capital_loss'] >= 1700).astype(int)\n","    df['capital_gain'] = df['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n","    \n","    return df\n","\n","# 4) age 조정 함수\n","def age(df):\n","    df.loc[df['age'] < 20, 'age_range'] = '~20'\n","    df.loc[df['age'] >= 65, 'age_range'] = '~65'\n","\n","    down = 20\n","    for i in range(45//5):\n","        df.loc[(df['age'] >= down) & (df['age'] < down+5), 'age_range'] = str(down)+'~'+str(down+5)\n","        down += 5\n","\n","    df['age'] = df['age_range']\n","    df.drop(['age_range'], axis=1, inplace=True)\n","    \n","    return df\n","    \n","# 5) One-hot encoding은 만들지 않았다.\n","\n","# 6) edu_num 새 변수 만들기\n","def edu(df):\n","    df['edu_num_high'] = (df['education_num'] >= 13).astype(int)\n","    \n","    return df\n","\n","# 7) hpw 새 변수 만들기\n","    \n","def hpw(df):\n","    df['hpw_high'] = (df['hours_per_week'] >= 50).astype(int)\n","\n","    return df\n","\n","# 8) MinMaxScaler\n","def mm_feature(df, feature):\n","    mm_scaler = MinMaxScaler()\n","    \n","    df[feature] = mm_scaler.fit_transform(df[feature].values.reshape(-1,1))\n","    \n","    return df, mm_scaler\n","\n","# 9) target 분리: train은 하고, test는 안하므로 따로 만들겠다.\n","def target_handle(df):\n","    df['income'] = df['income_>50K']\n","    df.drop(['income_>50K','income_<=50K'], axis=1, inplace=True)\n","    \n","    y_df = df.income\n","    X_df = df.drop(['income'], axis=1, inplace=False)\n","    \n","    return X_df, y_df\n","\n","def main(df):\n","    \n","    df1 = col_reduction(df)\n","    df2 = mar_st(df1)\n","    df3 = capital(df2)\n","    df4 = age(df3)\n","    \n","    df5 = pd.get_dummies(df4)\n","    \n","    df6 = edu(df5)\n","    df_fin = hpw(df6)\n","    \n","    return df_fin"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDa1_VupK21_"},"source":["# 적용\n","## main: 1) ~ 7)\n","train = main(train)\n","X_test = main(test)\n","\n","## 8) minmax scaler\n","train, mm_scaler1 = mm_feature(train,'education_num')\n","train, mm_scaler2 = mm_feature(train,'hours_per_week')\n","\n","X_test['education_num'] = mm_scaler1.transform(X_test['education_num'].values.reshape(-1,1))\n","X_test['hours_per_week'] = mm_scaler2.transform(X_test['hours_per_week'].values.reshape(-1,1))\n","\n","## 9) X, y split\n","X_train, y_train = target_handle(train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_BWZud6CK21_"},"source":["# 데이터 전처리 기존과 동일\n","# 학습용 데이터 분할처리 (8:2)\n","\n","from sklearn.model_selection import train_test_split\n","\n","x_train, x_valid, y_train, y_valid = train_test_split(X_train, y_train,\n","                                                     test_size=.2,\n","                                                     random_state = 42,\n","                                                     shuffle=True,\n","                                                     stratify = y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3cfxCGdZK21_","outputId":"d375fa4e-3102-4ab6-bc89-49420bc8f293"},"source":["print(x_train.shape)\n","print(y_train.shape)\n","print('='*50)\n","print(x_valid.shape)\n","print(y_valid.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(20839, 42)\n","(20839,)\n","==================================================\n","(5210, 42)\n","(5210,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T7NvNIviK22B"},"source":["# ML 모델 적용하기"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"9tAEKiF_K22B","outputId":"8574a514-7284-44de-f83f-5b86eee47e77"},"source":["# XGBoost 모델 사용\n","\n","import xgboost as xgb\n","\n","xgb_model = xgb.XGBClassifier()\n","xgb_model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n","              colsample_bynode=None, colsample_bytree=None, gamma=None,\n","              gpu_id=None, importance_type='gain', interaction_constraints=None,\n","              learning_rate=None, max_delta_step=None, max_depth=None,\n","              min_child_weight=None, missing=nan, monotone_constraints=None,\n","              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n","              random_state=None, reg_alpha=None, reg_lambda=None,\n","              scale_pos_weight=None, subsample=None, tree_method=None,\n","              validate_parameters=None, verbosity=None)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"RAvmXuVHK22C"},"source":["xgb_model.fit(x_train, y_train)\n","y_pred = xgb_model.predict(x_valid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"5jjms4JtK22C","outputId":"b25a97e4-b33b-49d8-ea46-7c6f6242678a"},"source":["from sklearn.metrics import f1_score\n","from sklearn.metrics import accuracy_score\n","\n","f1 = f1_score(y_valid, y_pred, average='micro')\n","print('-F1 Score: ', f1)\n","# print(f\"XGBClassifier\\n -F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")\n","\n","accuracy = accuracy_score(y_valid, y_pred)\n","print('-Accuracy score: ', accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-F1 Score:  0.87063339731286\n","-Accuracy score:  0.8706333973128599\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H-L8CHVWK22C"},"source":["### XGBoost 알고리즘의 개념 이해\n","XGBoost는 Gradient Boosting 알고리즘을 분산환경에서도 실행할 수 있도록 구현해놓은 라이브러리이다. \n","\n","즉, 앙상블 부스팅(ensemble boosting)의 특징인 가중치 부여를 경사하강법(gradient descent)으로 한다\n","\n","* xgboost의 특징\n","\n","    - gbm보다는 빠르다. (gbm보다 빠른 것입니다.)\n","    - 과적합(overfitting) 방지가 가능한 규제가 포함되어 있다.\n","    - CART(Classification And Regression Tree)를 기반으로 한다. 즉, 분류와 회귀가 둘 다 가능하다\n","    - 조기 종료(early stopping)을 제공한다."]},{"cell_type":"markdown","metadata":{"id":"nFYNK1AbK22C"},"source":["#### xgboost의 하이퍼파라미터(xgboost hyperparameter)\n","https://xgboost.readthedocs.io/en/latest/parameter.html\n","    \n","- n_estimators(혹은 num_boost_round) : 결정 트리의 개수\n","- max_depth : 트리의 깊이\n","- colsample_bytree : 컬럼의 샘플링 비율(random forest의 max_features와 비슷)\n","- subsample : weak learner가 학습에 사용하는 데이터 샘플링 비율\n","- learning_rete : 학습률\n","- min_split_loss :  리프 노드를 추가적으로 나눌지 결정하는 값\n","- reg_lambda : L2 규제\n","- reg_alpha : L1 규제"]},{"cell_type":"markdown","metadata":{"id":"ibdcKIknK22C"},"source":["from sklearn.model_selection import KFold\n","from sklearn.model_selection import GridSearchCV\n","# 파라미터 튜닝 (GridSearchCV)\n","\n","\n","kf = KFold(n_splits=5)\n","xgb_model = xgb.XGBClassifier()\n","\n","param_grid={'max_depth':range(3,10,3), 'min_child_weight':range(1,6,2)}\n","\n","# param_grid={'max_depth':range(3,10,3), 'min_child_weight':range(1,6,2), 'gamma':[i/10.0 for i in range(0,5)], \n","#             'subsample':[i/10.0 for i in range(6,10)], 'colsample_bytree':[i/10.0 for i in range(6,10)],\n","#             'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]}\n","\n","grid_sv = GridSearchCV(estimator=xgb.XGBClassifier(learning_rate =0.1, n_estimators=1000, \n","                        objective= 'binary:logistic'), \n","                       param_grid=param_grid, scoring='neg_mean_squared_error')\n","grid_sv.fit(x_train, y_train )\n","print(\"Best 파라미터 :\", grid_sv.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"aLSC2TiiK22C"},"source":["params = grid_sv.best_params_\n","\n","model = xgb.XGBClassifier(**params)\n","model.fit(x_train, y_train)\n","\n","mse = mean_squared_error(y_test, reg.predict(X_test))\n","print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))"]},{"cell_type":"code","metadata":{"id":"gPYSjCgKK22C"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ouf8zcjwK22C"},"source":["xgb1 = xgb.XGBClassifier(\n","    learning_rate =0.05,\n","    n_estimators=1000,\n","    max_depth=8,\n","    min_child_weight=3,\n","    gamma=5,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    objective= 'binary:logistic',\n","    nthread=-1,\n","    scale_pos_weight=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZ-DLInTK22C","outputId":"e74c9db2-c464-4ec5-e2f0-9f254afb8d64"},"source":["xgb1.fit(x_train, y_train)\n","\n","y_pred1 = xgb1.predict(x_valid)\n","f1 = f1_score(y_valid, y_pred1, average='micro')\n","print(f1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.8698656429942418\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jK4MrRFDK22C"},"source":["xgb2 = xgb.XGBClassifier(booster='gbtree', \n","    learning_rate =0.05,\n","    n_estimators=1000,\n","    max_depth=5,\n","    min_child_weight=3,\n","    gamma=5,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    objective= 'binary:logistic',\n","    nthread=-1,\n","    n_jobs=3,\n","    scale_pos_weight=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DaMnlv9UK22C","outputId":"4b1953b3-a428-41e7-e2f2-30456aedf3cb"},"source":["xgb2.fit(x_train, y_train)\n","\n","y_pred1 = xgb2.predict(x_valid)\n","f1 = f1_score(y_valid, y_pred1, average='micro')\n","print(f1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.8700575815738963\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8zbI3Vk4K22D"},"source":["### LightGBM"]},{"cell_type":"markdown","metadata":{"id":"QkByxUgpK22D"},"source":["LightGBM 적용\n","https://nurilee.com/lightgbm-definition-parameter-tuning/\n","    \n","Light GBM은 Gradient Boosting 프레임워크로 Tree 기반 학습 알고리즘으로 Tree가 수직적으로 확장(leaf-wise)되는 방식이다.\n","확장하기 위해서 max delta loss를 가진 leaf를 선택하게 되는데, \n","동일한 leaf를 확장할 때, leaf-wise 알고리즘은 level-wise 알고리즘(수평적 확장)보다 더 많은 loss, 손실을 줄일 수 있다.\n","LGBM은 또한 GPU 학습을 지원하기 때문에 속도가 빠르다.\n","\n","단, Light GBM은 overfitting (과적합)에 민감하고 작은 데이터에 대해서 과적합하기 쉽기 때문에 적은 데이터에는 사용하지 않는 것을 권한다.\n"]},{"cell_type":"markdown","metadata":{"id":"350oF0SfK22D"},"source":["#### LightGBM 파라미터\n","\n","https://lightgbm.readthedocs.io/en/latest/Parameters.html"]},{"cell_type":"markdown","metadata":{"id":"aYwB5T1oK22D"},"source":["* 더 빠른 속도\n","    - bagging_fraction\n","    - max_bin은 작게\n","    - save_binary를 쓰면 데이터 로딩속도가 빨라짐\n","    - parallel learning 사용\n","\n","* 더 높은 정확도\n","    - max_bin을 크게\n","    - num_iterations 는 크게하고 learning_rate는 작게\n","    - num_leaves를 크게(과적합의 원인이 될 수 있음)\n","    - boosting 알고리즘 'dart' 사용\n","\n","* 과적합을 줄이기\n","    - max_bin을 작게\n","    - num_leaves를 작게\n","    - min_data_in_leaf와 min_sum_hessian_in_leaf 사용하기"]},{"cell_type":"code","metadata":{"id":"xVu0xjEBK22D","outputId":"c5fb2ff3-b3ce-4a99-f91e-d2ad674d2d02"},"source":["from lightgbm import LGBMClassifier\n","\n","lgb_model = LGBMClassifier(n_estimators=400)\n","lgb_model.fit(x_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LGBMClassifier(n_estimators=400)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"29KXrCMCK22D","outputId":"dec7456c-71fd-415c-b3a0-9de1fd5ea6a2"},"source":["pred = lgb_model.predict(x_valid)\n","\n","f1 = f1_score(y_valid, pred, average='micro')\n","print('-F1 Score: ', f1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-F1 Score:  0.8664107485604606\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"AoGQIl-fK22D","outputId":"fd22a41e-6062-4a97-bde9-fe4bad9bd12f"},"source":["import lightgbm as lgb\n","d_train = lgb.Dataset(x_train, label=y_train)\n","d_test = lgb.Dataset(x_valid, label=y_valid)\n","params = {}\n","params['learning_rate'] = 0.003\n","params['boosting_type'] = 'gbdt'\n","params['objective'] = 'binary'\n","params['metric'] = 'binary_logloss'\n","params['sub_feature'] = 0.5\n","params['num_leaves'] = 10\n","params['min_data'] = 50\n","params['max_depth'] = 16\n","params['is_training_metric'] = True\n","clf = lgb.train(params, d_train, 1000, d_test, verbose_eval=100, early_stopping_rounds=100)\n","\n","y_pred = clf.predict(x_valid)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[LightGBM] [Info] Number of positive: 5044, number of negative: 15795\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001924 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 324\n","[LightGBM] [Info] Number of data points in the train set: 20839, number of used features: 41\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242046 -> initscore=-1.141494\n","[LightGBM] [Info] Start training from score -1.141494\n","Training until validation scores don't improve for 100 rounds\n","[100]\tvalid_0's binary_logloss: 0.483689\n","[200]\tvalid_0's binary_logloss: 0.438894\n","[300]\tvalid_0's binary_logloss: 0.406864\n","[400]\tvalid_0's binary_logloss: 0.384137\n","[500]\tvalid_0's binary_logloss: 0.367395\n","[600]\tvalid_0's binary_logloss: 0.35422\n","[700]\tvalid_0's binary_logloss: 0.343499\n","[800]\tvalid_0's binary_logloss: 0.335146\n","[900]\tvalid_0's binary_logloss: 0.328165\n","[1000]\tvalid_0's binary_logloss: 0.322308\n","Did not meet early stopping. Best iteration is:\n","[1000]\tvalid_0's binary_logloss: 0.322308\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AvTKjcTfK22D"},"source":["# f1 = f1_score(y_valid, y_pred, average='micro')\n","# print('-F1 Score: ', f1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n27308NbK22D"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"46cEEkcCT9dV"},"source":["from sklearn.model_selection import KFold from sklearn.model_selection import GridSearchCV\n","\n","# 파라미터 튜닝 (GridSearchCV)\n","kf = KFold(n_splits=5) xgb_model = xgb.XGBClassifier()\n","\n","param_grid={'max_depth':range(3,10,3), 'min_child_weight':range(1,6,2)}\n","\n","param_grid={'max_depth':range(3,10,3), 'min_child_weight':range(1,6,2), 'gamma':[i/10.0 for i in range(0,5)],\n","'subsample':[i/10.0 for i in range(6,10)], 'colsample_bytree':[i/10.0 for i in range(6,10)],\n","'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]}\n","grid_sv = GridSearchCV(estimator=xgb.XGBClassifier(learning_rate =0.1, \n","                                                   n_estimators=1000, \n","                                                   objective= 'binary:logistic'), \n","                                                   param_grid=param_grid, \n","                                                   scoring='neg_mean_squared_error') \n","\n","grid_sv.fit(x_train, y_train ) print(\"Best 파라미터 :\", grid_sv.best_params_)"],"execution_count":null,"outputs":[]}]}